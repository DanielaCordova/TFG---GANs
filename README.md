# Generation of Content Through Antagonic Generative Networks

There have been major advances in artificial intelligence, particularly machine learning, over the past decade. These breakthroughs have been made mostly because of the increase in processing capacity over time, as anticipated by Moore's law in 1965, which projected that the number of transistors in a given surface would double every year, as illustrated in the next figure. 

![moores-law](https://user-images.githubusercontent.com/60478676/170839679-8f8ad6e0-e5a8-4c23-8b5a-05617620ffd5.jpg)


This increase in computational power has allowed the implementation of many techniques that had been considered previously, but had not been tested to their full potential due to the lack of computational power. For example, although multilayer perceptrons were proven to be able to approach any mathematical function, by the 1980s they were outclassed by support vector machines due to the inefficiency in the training process. Thanks to advancements in efficiency and computational power, multilayer perceptrons have grown into far more complicated structures like the ones presented in this work.

This increase in computation power has been used in recent years to solve mostly classification and regression problems using a variety of algorithms and models, in which the model must learn either to differentiate a sample between every class available or how the features of a given sample are related to another in order to predict its value. However, the aim of this work is to solve a much more complex problem. The models to be investigated in this work must understand not only the distinctions between the classes to which a sample can belong but also the exact attributes that essentially characterize each conceivable class. For example, while differentiating a cat from a dog is relatively simple, defining the specific characteristics of a dog and a cat is a considerably more difficult process. These models learn those specific properties to generate synthetic samples that incorporate the learning of the model.

There has been a lot of progress in recent years when it comes to creating videos, photos, and even music. Deepfakes are great examples of these advances: images or films of individuals that have been artificially made by transferring the facial expressions and gestures from one video to another, resulting in videos of people saying things they did not actually say. Other studies worth examining are NVIDIA's GauGAN (https://arxiv.org/abs/1903.07291)which can transform a segmentation map into a realistic image, and OpenAI's Dall E (https://doi.org/10.48550/arxiv.2102.12092), which can generate faithful images based on a textual description provided by the user.  This next figures illustrate examples of what these two models can do.



<p float="left">
  <img src="https://user-images.githubusercontent.com/60478676/170839738-d260e68d-2c81-47cc-acd0-f3725ca0173b.png" width="300">
  <img src="https://user-images.githubusercontent.com/60478676/170839740-f08bf15f-5900-49a8-9ba5-615f7c492019.jpg" width="300">
  <img src="https://user-images.githubusercontent.com/60478676/170839742-b45be7f9-0322-43be-b7ec-390bbaaa808a.png" width="300">
</p>




This is why image processing specialists are increasingly paying attention to the potential of generative adversarial networks (GANs). Image scaling, image shifting between domains (e.g. transitioning from daylight to night scenes), and many other applications benefit greatly from the usage of GANs in image production. To achieve these results, many modified GAN architectures have been developed with their own distinct properties for solving certain image processing challenge, although the baseline always stays the same.

In a GAN, two agents fight against each other: a Generator and a Discriminator, shown here

![ejemplo1](https://user-images.githubusercontent.com/60478676/170839850-ed761571-4221-4688-b8db-fb37d2efc675.png)


The Generator produces an image that tries to mimic a real one, and then, that image is fed to the Discriminator, so it determines whether the image generated by the Generator is authentic or not. Initially, the Generator will produce low-quality images that the Discriminator will immediately identify as fake. Thanks to the Discriminator's decision, the Generator will learn to trick the Discriminator after collecting enough information, while the Discriminator will learn what a real image looks like by processing several real images. As a consequence, the generative model ends up producing highly realistic outcomes.



## Github Structure


The repository is structured the following way:
    
Each Model is stored on its own folder, these folders contain 4 main files: 
    
- {Model}Generator.py : this contains the class of the generator for said model.
        
- {Model}Discrimitor.py : the same as the previous one, this file contains the discriminator for the models.
        
- trining{Model}.py : this file contains the code capable of training these models, all the training details can be changed within it, such as the size of the image generated, the dataset used for its training, and if needed the previously trained model that it loads for it to resume its training from a previous run, among others.
        
- generateSamples.py : this file contains the code capable of generating samples from a previously trained model. Like the last file, within it you can change the aspects of the generated images to an extent, as it has to match the generator that it is using.

