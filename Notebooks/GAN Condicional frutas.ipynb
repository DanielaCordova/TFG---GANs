{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1czVdIlqnImH"
   },
   "source": [
    "# Build a Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KD3ZgLs80vY"
   },
   "source": [
    "### Goals\n",
    "In this notebook, you're going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.\n",
    "\n",
    "You'll then do some exploration of the generated images to visualize what the noise and class vectors mean.  \n",
    "\n",
    "### Learning Objectives\n",
    "1.   Learn the technical difference between a conditional and unconditional GAN.\n",
    "2.   Understand the distinction between the class and noise vector in a conditional GAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSJ3lVizIR26"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "For this assignment, you will be using the MNIST dataset again, but there's nothing stopping you from applying this generator code to produce images of animals conditioned on the species or pictures of faces conditioned on facial characteristics.\n",
    "\n",
    "Note that this assignment requires no changes to the architectures of the generator or discriminator, only changes to the data passed to both. The generator will no longer take `z_dim` as an argument, but  `input_dim` instead, since you need to pass in both the noise and class vectors. In addition to good variable naming, this also means that you can use the generator and discriminator code you have previously written with different parameters.\n",
    "\n",
    "You will begin by importing the necessary libraries and building the generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbRUCE0TK2fV"
   },
   "source": [
    "#### Packages and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfkorNJrnmNO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as tt\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(3, 28, 28), nrow=5, show=True):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1A1M6kpnfxw"
   },
   "source": [
    "#### Generator and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvO7h0LYnEJZ"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=10, im_chan=3, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 131, 1, 1]' is invalid for input of size 4704",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_472/1211455446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m131\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# make a forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# remove these hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_472/1657324645.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, noise)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mnoise\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         '''\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 131, 1, 1]' is invalid for input of size 4704"
     ]
    }
   ],
   "source": [
    "summary(Generator(131, 3,64 ), input_size=(3, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9fScH98nkYH"
   },
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aA4AxGnmpuPq"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      im_chan: the number of channels of the output image, a scalar\n",
    "            (MNIST is black-and-white, so 1 channel is your default)\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSBva5ffM5KT"
   },
   "source": [
    "## Class Input\n",
    "\n",
    "In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0's and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">get_one_hot_labels</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   This code can be done in one line.\n",
    "2.   The documentation for [F.one_hot](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot) may be helpful.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81_1g6odOeV5"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_one_hot_labels\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    return F.one_hot(labels,n_classes)\n",
    "    #### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLM5a64HWeqX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    get_one_hot_labels(\n",
    "        labels=torch.Tensor([[0, 2, 1]]).long(),\n",
    "        n_classes=3\n",
    "    ).tolist() == \n",
    "    [[\n",
    "      [1, 0, 0], \n",
    "      [0, 0, 1], \n",
    "      [0, 1, 0]\n",
    "    ]]\n",
    ")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DSarQYONAxR"
   },
   "source": [
    "Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.\n",
    "\n",
    "To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help.\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">combine_vectors</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   This code can also be written in one line.\n",
    "2.   The documentation for [torch.cat](https://pytorch.org/docs/master/generated/torch.cat.html) may be helpful.\n",
    "3.   Specifically, you might want to look at what the `dim` argument of `torch.cat` does.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbwITPc0M6uh"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: combine_vectors\n",
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    # Note: Make sure this function outputs a float no matter what inputs it receives\n",
    "    #### START CODE HERE ####\n",
    "    combined = torch.cat((x.float(),y.float()), 1)\n",
    "    #### END CODE HERE ####\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwpMxamzXcbA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "combined = combine_vectors(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]));\n",
    "# Check exact order of elements\n",
    "assert torch.all(combined == torch.tensor([[1, 2, 5, 6], [3, 4, 7, 8]]))\n",
    "# Tests that items are of float type\n",
    "assert (type(combined[0][0].item()) == float)\n",
    "# Check shapes\n",
    "combined = combine_vectors(torch.randn(1, 4, 5), torch.randn(1, 8, 5));\n",
    "assert tuple(combined.shape) == (1, 12, 5)\n",
    "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12).long()).shape) == (1, 30, 12)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sh98CdJXY_SB"
   },
   "source": [
    "## Training\n",
    "Now you can start to put it all together!\n",
    "First, you will define some new parameters:\n",
    "\n",
    "*   mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it's black-and-white) so 1 x 28 x 28\n",
    "*   n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGRk6NB4ZqJ8"
   },
   "outputs": [],
   "source": [
    "mnist_shape = (3, 28, 28)\n",
    "n_classes = 131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cw3x2kHZd6L"
   },
   "source": [
    "And you also include the same parameters from previous assignments:\n",
    "\n",
    "  *   criterion: the loss function\n",
    "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
    "  *   z_dim: the dimension of the noise vector\n",
    "  *   display_step: how often to display/visualize the images\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   lr: the learning rate\n",
    "  *   device: the device type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GZt2cxlZRQw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpetas : ['LICENSE', 'papers', 'readme.md', 'Test', 'test-multiple_fruits', 'Training']\n",
      "******************************************************************************\n",
      "131 clases : ['Apple Braeburn', 'Apple Crimson Snow', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Pink Lady', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow 1', 'Apple Red Yellow 2', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Lady Finger', 'Banana Red', 'Beetroot', 'Blueberry', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cauliflower', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Chestnut', 'Clementine', 'Cocos', 'Corn', 'Corn Husk', 'Cucumber Ripe', 'Cucumber Ripe 2', 'Dates', 'Eggplant', 'Fig', 'Ginger Root', 'Granadilla', 'Grape Blue', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grape White 3', 'Grape White 4', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Hazelnut', 'Huckleberry', 'Kaki', 'Kiwi', 'Kohlrabi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Mango Red', 'Mangostan', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Nectarine Flat', 'Nut Forest', 'Nut Pecan', 'Onion Red', 'Onion Red Peeled', 'Onion White', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach 2', 'Peach Flat', 'Pear', 'Pear 2', 'Pear Abate', 'Pear Forelle', 'Pear Kaiser', 'Pear Monster', 'Pear Red', 'Pear Stone', 'Pear Williams', 'Pepino', 'Pepper Green', 'Pepper Orange', 'Pepper Red', 'Pepper Yellow', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Plum 2', 'Plum 3', 'Pomegranate', 'Pomelo Sweetie', 'Potato Red', 'Potato Red Washed', 'Potato Sweet', 'Potato White', 'Quince', 'Rambutan', 'Raspberry', 'Redcurrant', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Tomato 1', 'Tomato 2', 'Tomato 3', 'Tomato 4', 'Tomato Cherry Red', 'Tomato Heart', 'Tomato Maroon', 'Tomato not Ripened', 'Tomato Yellow', 'Walnut', 'Watermelon']\n",
      "Tama単o del dataset de entrenamiento : 67692\n",
      "Tama単o del dataset para test : 22688\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Resize(28)\n",
    "])\n",
    "\n",
    "data_dir = './fruits-360_dataset/fruits-360'\n",
    "print('Carpetas :', os.listdir(data_dir))\n",
    "print(\"******************************************************************************\")\n",
    "classes = os.listdir(data_dir + \"/Training\")\n",
    "print('131 clases :', classes)\n",
    "\n",
    "\n",
    "dataset = ImageFolder(data_dir + '/Training', transform=transform) ##El transform anterior oscurece las imagenes\n",
    "print('Tama単o del dataset de entrenamiento :', len(dataset))\n",
    "test = ImageFolder(data_dir + '/Test', transform=transform)\n",
    "print('Tama単o del dataset para test :', len(test))\n",
    "\n",
    "\n",
    "val_size = len(dataset)//10\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds) # train_ds length = dataset length - val_ds length\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,shuffle=True)\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size*2, num_workers=4, pin_memory=True)\n",
    "#dataloader = DataLoader(\n",
    "#    MNIST('.', download=False, transform=transform),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6sxYaNRbRjB"
   },
   "source": [
    "Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcTP43grUD5z"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_input_dimensions\n",
    "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
    "    '''\n",
    "    Function for getting the size of the conditional input dimensions \n",
    "    from z_dim, the image shape, and number of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "                (10 for MNIST)\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 28 x 28 for MNIST)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_im_chan = mnist_shape[0] + n_classes\n",
    "    #### END CODE HERE ####\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YouJSJ5cVJvD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def test_input_dims():\n",
    "    gen_dim, disc_dim = get_input_dimensions(23, (12, 23, 52), 9)\n",
    "    assert gen_dim == 32\n",
    "    assert disc_dim == 21\n",
    "test_input_dims()\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXptQZcwrBrq"
   },
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfcEHPwCQYLY"
   },
   "source": [
    "Now to train, you would like both your generator and your discriminator to know what class of image should be generated. There are a few locations where you will need to implement code.\n",
    "\n",
    "For example, if you're generating a picture of the number \"1\", you would need to:\n",
    "  \n",
    "1.   Tell that to the generator, so that it knows it should be generating a \"1\"\n",
    "2.   Tell that to the discriminator, so that it knows it should be looking at a \"1\". If the discriminator is told it should be looking at a 1 but sees something that's clearly an 8, it can guess that it's probably fake\n",
    "\n",
    "There are no explicit unit tests here -- if this block of code runs and you don't change any of the other variables, then you've done it correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB3hUwWTbVJC",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f44b38687de44b6859c65421a680e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_472/2571160990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Dataloader returns the batches and the labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mcur_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Flatten the batch of real images from the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m    231\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GANS\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED CELL\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "#UNIT TEST NOTE: Initializations needed for grading\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size \n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        # Now you can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "       \n",
    "        #### START CODE HERE ####\n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "        # Check that correct tensors were combined\n",
    "        assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1])\n",
    "        # It comes from the correct generator\n",
    "        assert tuple(fake.shape) == (len(real), 3, 28, 28)\n",
    "\n",
    "        # Now you can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels, \n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "        \n",
    "        #### START CODE HERE ####\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        # Make sure shapes are correct \n",
    "        assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n",
    "        assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "        # Shapes must match\n",
    "        assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)\n",
    "        assert tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)\n",
    "        \n",
    "        \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "        #\n",
    "\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (3, 64, 64)\n",
    "n_classes = 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpetas : ['LICENSE', 'papers', 'readme.md', 'Test', 'test-multiple_fruits', 'Training']\n",
      "******************************************************************************\n",
      "131 clases : ['Apple Braeburn', 'Apple Crimson Snow', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Pink Lady', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow 1', 'Apple Red Yellow 2', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Lady Finger', 'Banana Red', 'Beetroot', 'Blueberry', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cauliflower', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Chestnut', 'Clementine', 'Cocos', 'Corn', 'Corn Husk', 'Cucumber Ripe', 'Cucumber Ripe 2', 'Dates', 'Eggplant', 'Fig', 'Ginger Root', 'Granadilla', 'Grape Blue', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grape White 3', 'Grape White 4', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Hazelnut', 'Huckleberry', 'Kaki', 'Kiwi', 'Kohlrabi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Mango Red', 'Mangostan', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Nectarine Flat', 'Nut Forest', 'Nut Pecan', 'Onion Red', 'Onion Red Peeled', 'Onion White', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach 2', 'Peach Flat', 'Pear', 'Pear 2', 'Pear Abate', 'Pear Forelle', 'Pear Kaiser', 'Pear Monster', 'Pear Red', 'Pear Stone', 'Pear Williams', 'Pepino', 'Pepper Green', 'Pepper Orange', 'Pepper Red', 'Pepper Yellow', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Plum 2', 'Plum 3', 'Pomegranate', 'Pomelo Sweetie', 'Potato Red', 'Potato Red Washed', 'Potato Sweet', 'Potato White', 'Quince', 'Rambutan', 'Raspberry', 'Redcurrant', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Tomato 1', 'Tomato 2', 'Tomato 3', 'Tomato 4', 'Tomato Cherry Red', 'Tomato Heart', 'Tomato Maroon', 'Tomato not Ripened', 'Tomato Yellow', 'Walnut', 'Watermelon']\n",
      "Tama単o del dataset de entrenamiento : 67692\n",
      "Tama単o del dataset para test : 22688\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64*2\n",
    "display_step = 500\n",
    "batch_size = 16\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Resize(64)\n",
    "])\n",
    "\n",
    "data_dir = './fruits-360_dataset/fruits-360'\n",
    "print('Carpetas :', os.listdir(data_dir))\n",
    "print(\"******************************************************************************\")\n",
    "classes = os.listdir(data_dir + \"/Training\")\n",
    "print('131 clases :', classes)\n",
    "\n",
    "\n",
    "dataset = ImageFolder(data_dir + '/Training', transform=transform) ##El transform anterior oscurece las imagenes\n",
    "print('Tama単o del dataset de entrenamiento :', len(dataset))\n",
    "test = ImageFolder(data_dir + '/Test', transform=transform)\n",
    "print('Tama単o del dataset para test :', len(test))\n",
    "\n",
    "\n",
    "val_size = len(dataset)//10\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds) # train_ds length = dataset length - val_ds length\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,shuffle=True)\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size*2, num_workers=4, pin_memory=True)\n",
    "#dataloader = DataLoader(\n",
    "#    MNIST('.', download=False, transform=transform),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=10, im_chan=3, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 16),\n",
    "            self.make_gen_block(hidden_dim * 16, hidden_dim * 8),\n",
    "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Conv2d(output_channels, output_channels, 13, stride=1),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Sequential: 1-1                        --\n",
      "|    Sequential: 2-1                   --\n",
      "|    |    ConvTranspose2d: 3-1         1,208,320\n",
      "|    |    BatchNorm2d: 3-2             2,048\n",
      "|    |    ReLU: 3-3                    --\n",
      "|    Sequential: 2-2                   --\n",
      "|    |    ConvTranspose2d: 3-4         4,719,104\n",
      "|    |    BatchNorm2d: 3-5             1,024\n",
      "|    |    ReLU: 3-6                    --\n",
      "|    Sequential: 2-3                   --\n",
      "|    |    ConvTranspose2d: 3-7         1,179,904\n",
      "|    |    BatchNorm2d: 3-8             512\n",
      "|    |    ReLU: 3-9                    --\n",
      "|    Sequential: 2-4                   --\n",
      "|    |    ConvTranspose2d: 3-10        524,416\n",
      "|    |    BatchNorm2d: 3-11            256\n",
      "|    |    ReLU: 3-12                   --\n",
      "|    Sequential: 2-5                   --\n",
      "|    |    ConvTranspose2d: 3-13        73,792\n",
      "|    |    BatchNorm2d: 3-14            128\n",
      "|    |    ReLU: 3-15                   --\n",
      "|    Sequential: 2-6                   --\n",
      "|    |    ConvTranspose2d: 3-16        3,075\n",
      "|    |    Conv2d: 3-17                 1,524\n",
      "|    |    Tanh: 3-18                   --\n",
      "=================================================================\n",
      "Total params: 7,714,103\n",
      "Trainable params: 7,714,103\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential: 1-1                        --\n",
       "|    Sequential: 2-1                   --\n",
       "|    |    ConvTranspose2d: 3-1         1,208,320\n",
       "|    |    BatchNorm2d: 3-2             2,048\n",
       "|    |    ReLU: 3-3                    --\n",
       "|    Sequential: 2-2                   --\n",
       "|    |    ConvTranspose2d: 3-4         4,719,104\n",
       "|    |    BatchNorm2d: 3-5             1,024\n",
       "|    |    ReLU: 3-6                    --\n",
       "|    Sequential: 2-3                   --\n",
       "|    |    ConvTranspose2d: 3-7         1,179,904\n",
       "|    |    BatchNorm2d: 3-8             512\n",
       "|    |    ReLU: 3-9                    --\n",
       "|    Sequential: 2-4                   --\n",
       "|    |    ConvTranspose2d: 3-10        524,416\n",
       "|    |    BatchNorm2d: 3-11            256\n",
       "|    |    ReLU: 3-12                   --\n",
       "|    Sequential: 2-5                   --\n",
       "|    |    ConvTranspose2d: 3-13        73,792\n",
       "|    |    BatchNorm2d: 3-14            128\n",
       "|    |    ReLU: 3-15                   --\n",
       "|    Sequential: 2-6                   --\n",
       "|    |    ConvTranspose2d: 3-16        3,075\n",
       "|    |    Conv2d: 3-17                 1,524\n",
       "|    |    Tanh: 3-18                   --\n",
       "=================================================================\n",
       "Total params: 7,714,103\n",
       "Trainable params: 7,714,103\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Generator(131, 3,64 ), input_size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50546183bea34392aafd64df360ef9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_InputVector_paraEtiquetar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_472/3533421730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mreal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mvectorEtiquetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_InputVector_paraEtiquetar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mimg_vectorEtiquetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorEtiquetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mimg_vectorEtiquetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_vectorEtiquetas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDimImg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDimImg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##img_vectorEtiquetas.repeat(1, 1, 100, 100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_InputVector_paraEtiquetar' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "perdidasGenerador = []\n",
    "perdidasDiscriminador = []\n",
    "\n",
    "#Inicializaciones necesarias\n",
    "ruido_y_etiq = False\n",
    "Fake = False\n",
    "\n",
    "imgFalsa_y_etiq = False\n",
    "imgReales_y_etiq = False\n",
    "prediccionFalsaDisc = False\n",
    "prediccionRealDisc = False\n",
    "##(3, 100, 100)\n",
    "def ver_tensor_img(img_tensor, num_images=25, size=(3, 64, 64), nrow=5, show=True): ##para ver las imagenes en una grid\n",
    "    img_tensor = (img_tensor + 1) / 2\n",
    "    image_unflat = img_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    for real, labels in tqdm(dataloader):  ##tqdm -> progress bar \n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        vectorEtiquetas = get_InputVector_paraEtiquetar(labels.to(device), numClases)\n",
    "        img_vectorEtiquetas = vectorEtiquetas[:, :, None, None] \n",
    "        img_vectorEtiquetas = img_vectorEtiquetas.repeat(1, 1, DimImg[1], DimImg[2]) ##img_vectorEtiquetas.repeat(1, 1, 100, 100)\n",
    "\n",
    "        ### Actualizar discriminator ###\n",
    "        # Para cada mini-batch durante la fase de entrenamiento queremos establecer expl鱈citamente los gradientes en cero antes de comenzar \n",
    "        #la propagaci坦n hacia atr叩s\n",
    "        disc_opt.zero_grad()\n",
    "        # Ruido al lote actual\n",
    "        ruidoFake = getNoise(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        #Generador\n",
    "        ruido_y_etiq = combinarVectores(ruidoFake, vectorEtiquetas) #vectores de ruido . vector etiquetas\n",
    "        fake = gen(ruido_y_etiq) #Generar las imagenes falsas con Gen\n",
    "      \n",
    "  \n",
    "    \n",
    "        # Discriminador\n",
    "        imgFalsa_y_etiq = combinarVectores(fake, img_vectorEtiquetas) #Combinar imagenes falsas con el vector de etiq\n",
    "        imgReales_y_etiq = combinarVectores(real, img_vectorEtiquetas)#Combinar imagenes reales con el vector de etiq\n",
    "        prediccionFalsaDisc = disc(imgFalsa_y_etiq.detach()) #Prediccion del discriminador en las imagenes falsas\n",
    "        prediccionRealDisc = disc(imgReales_y_etiq) #Prediccion del discriminador en las imagenes reales\n",
    "\n",
    "        perdidasFalsasDisc = criterion(prediccionFalsaDisc, torch.zeros_like(prediccionFalsaDisc))\n",
    "        perdidasRealesDisc = criterion(prediccionRealDisc, torch.ones_like(prediccionRealDisc))\n",
    "        perdidaDisc = (perdidasFalsasDisc + perdidasRealesDisc) / 2\n",
    "        perdidaDisc.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "\n",
    "        #Perdida promedio\n",
    "        perdidasDiscriminador += [perdidaDisc.item()]\n",
    "  \n",
    "        ### Actualizar Generador ###\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        imgFalsa_y_etiq = combinarVectores(fake, img_vectorEtiquetas)\n",
    "        \n",
    "        prediccionFalsa = disc(imgFalsa_y_etiq)\n",
    "        perdidaGen = criterion(prediccionFalsa, torch.ones_like(prediccionFalsa))\n",
    "        perdidaGen.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        \n",
    "        perdidasGenerador += [perdidaGen.item()]\n",
    "        \n",
    "\n",
    "        ##Mostrar en pantalla\n",
    "        if i % display_step == 0 and i > 0:\n",
    "          promedioGen = sum(perdidasGenerador[-display_step:]) / display_step\n",
    "          promedioDisc = sum(perdidasDiscriminador[-display_step:]) / display_step\n",
    "          print(f\"Paso {i}: Perdidas del Generador: {promedioGen}, Perdidas del Discriminador: {promedioDisc}\")\n",
    "          ver_tensor_img(fake)\n",
    "          ver_tensor_img(real)\n",
    "          step_bins = 20\n",
    "          x_axis = sorted([i * step_bins for i in range(len(perdidasGenerador) // step_bins)] * step_bins)\n",
    "          numEjemplos = (len(perdidasGenerador) // step_bins) * step_bins\n",
    "          plt.plot(\n",
    "              range(numEjemplos // step_bins), \n",
    "              torch.Tensor(perdidasGenerador[:numEjemplos]).view(-1, step_bins).mean(1),\n",
    "              label=\"Generator loss:\"\n",
    "          )\n",
    "          plt.plot(\n",
    "              range(numEjemplos // step_bins), \n",
    "              torch.Tensor(perdidasDiscriminador[:numEjemplos]).view(-1, step_bins).mean(1),\n",
    "              label=\"discriminator loss:\"\n",
    "          )\n",
    "          plt.legend()\n",
    "          plt.show()\n",
    "        elif i == 0:\n",
    "            print(\"Inicio\")\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generador(nn.Module):\n",
    "    def __init__(self, noiseDim, condDim, numChan=3, hiddenDim=64):\n",
    "        super(Generador, self).__init__()\n",
    "        self.inDim = noiseDim  \n",
    "        self.gen = nn.Sequential(\n",
    "            self.generar_bloque_generador(self.inDim, hiddenDim * 64),\n",
    "            self.generar_bloque_generador(hiddenDim * 64, hiddenDim * 32),\n",
    "            self.generar_bloque_generador(hiddenDim * 32, hiddenDim * 16),\n",
    "            self.generar_bloque_generador(hiddenDim * 16, hiddenDim * 8),\n",
    "            self.generar_bloque_generador(hiddenDim * 8, hiddenDim * 4),\n",
    "            self.generar_bloque_generador(hiddenDim * 4, hiddenDim * 2, kernTam=4, stride=1),\n",
    "            self.generar_bloque_generador(hiddenDim * 2, hiddenDim),\n",
    "            self.generar_bloque_generador(hiddenDim, numChan, kernTam=4, ultimaCapa=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.view(len(input), self.inDim, 1, 1)\n",
    "        return self.gen(x)\n",
    "##kernTam=2 stride=4\n",
    "    def generar_bloque_generador(self, inChan, outChan, kernTam=3, stride=2, ultimaCapa=False):\n",
    "        if ultimaCapa:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(inChan, outChan, kernTam, stride),\n",
    "                nn.Conv2d(outChan, outChan, kernTam, stride=2),\n",
    "                nn.Conv2d(outChan, outChan, kernTam +2, stride=2),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(inChan, outChan, kernTam, stride),\n",
    "                nn.BatchNorm2d(outChan),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Sequential: 1-1                        --\n",
      "|    Sequential: 2-1                   --\n",
      "|    |    ConvTranspose2d: 3-1         4,833,280\n",
      "|    |    BatchNorm2d: 3-2             8,192\n",
      "|    |    ReLU: 3-3                    --\n",
      "|    Sequential: 2-2                   --\n",
      "|    |    ConvTranspose2d: 3-4         75,499,520\n",
      "|    |    BatchNorm2d: 3-5             4,096\n",
      "|    |    ReLU: 3-6                    --\n",
      "|    Sequential: 2-3                   --\n",
      "|    |    ConvTranspose2d: 3-7         18,875,392\n",
      "|    |    BatchNorm2d: 3-8             2,048\n",
      "|    |    ReLU: 3-9                    --\n",
      "|    Sequential: 2-4                   --\n",
      "|    |    ConvTranspose2d: 3-10        4,719,104\n",
      "|    |    BatchNorm2d: 3-11            1,024\n",
      "|    |    ReLU: 3-12                   --\n",
      "|    Sequential: 2-5                   --\n",
      "|    |    ConvTranspose2d: 3-13        1,179,904\n",
      "|    |    BatchNorm2d: 3-14            512\n",
      "|    |    ReLU: 3-15                   --\n",
      "|    Sequential: 2-6                   --\n",
      "|    |    ConvTranspose2d: 3-16        524,416\n",
      "|    |    BatchNorm2d: 3-17            256\n",
      "|    |    ReLU: 3-18                   --\n",
      "|    Sequential: 2-7                   --\n",
      "|    |    ConvTranspose2d: 3-19        73,792\n",
      "|    |    BatchNorm2d: 3-20            128\n",
      "|    |    ReLU: 3-21                   --\n",
      "|    Sequential: 2-8                   --\n",
      "|    |    ConvTranspose2d: 3-22        3,075\n",
      "|    |    Conv2d: 3-23                 147\n",
      "|    |    Conv2d: 3-24                 327\n",
      "|    |    Tanh: 3-25                   --\n",
      "=================================================================\n",
      "Total params: 105,725,213\n",
      "Trainable params: 105,725,213\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential: 1-1                        --\n",
       "|    Sequential: 2-1                   --\n",
       "|    |    ConvTranspose2d: 3-1         4,833,280\n",
       "|    |    BatchNorm2d: 3-2             8,192\n",
       "|    |    ReLU: 3-3                    --\n",
       "|    Sequential: 2-2                   --\n",
       "|    |    ConvTranspose2d: 3-4         75,499,520\n",
       "|    |    BatchNorm2d: 3-5             4,096\n",
       "|    |    ReLU: 3-6                    --\n",
       "|    Sequential: 2-3                   --\n",
       "|    |    ConvTranspose2d: 3-7         18,875,392\n",
       "|    |    BatchNorm2d: 3-8             2,048\n",
       "|    |    ReLU: 3-9                    --\n",
       "|    Sequential: 2-4                   --\n",
       "|    |    ConvTranspose2d: 3-10        4,719,104\n",
       "|    |    BatchNorm2d: 3-11            1,024\n",
       "|    |    ReLU: 3-12                   --\n",
       "|    Sequential: 2-5                   --\n",
       "|    |    ConvTranspose2d: 3-13        1,179,904\n",
       "|    |    BatchNorm2d: 3-14            512\n",
       "|    |    ReLU: 3-15                   --\n",
       "|    Sequential: 2-6                   --\n",
       "|    |    ConvTranspose2d: 3-16        524,416\n",
       "|    |    BatchNorm2d: 3-17            256\n",
       "|    |    ReLU: 3-18                   --\n",
       "|    Sequential: 2-7                   --\n",
       "|    |    ConvTranspose2d: 3-19        73,792\n",
       "|    |    BatchNorm2d: 3-20            128\n",
       "|    |    ReLU: 3-21                   --\n",
       "|    Sequential: 2-8                   --\n",
       "|    |    ConvTranspose2d: 3-22        3,075\n",
       "|    |    Conv2d: 3-23                 147\n",
       "|    |    Conv2d: 3-24                 327\n",
       "|    |    Tanh: 3-25                   --\n",
       "=================================================================\n",
       "Total params: 105,725,213\n",
       "Trainable params: 105,725,213\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Generator(131, 3,64 ), input_size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminador(nn.Module):\n",
    "\n",
    "  def __init__(self, inChan=1, hiddenDim=64):\n",
    "    super(Discriminador, self).__init__()\n",
    "    self.discriminador = nn.Sequential(\n",
    "        self.generar_bloque_discriminador(inChan, hiddenDim),\n",
    "        self.generar_bloque_discriminador(hiddenDim, hiddenDim * 2),\n",
    "        self.generar_bloque_discriminador(hiddenDim*2, inChan,ultimaCapa= True )\n",
    "    )\n",
    "\n",
    "  def generar_bloque_discriminador(self, inChan, outChan, kernel = 4, stride = 2, ultimaCapa = False):\n",
    "    if ultimaCapa :\n",
    "      return nn.Sequential(\n",
    "          nn.Conv2d(inChan, outChan, kernel, stride)\n",
    "      )\n",
    "    else :\n",
    "      return nn.Sequential(\n",
    "          nn.Conv2d(inChan, outChan, kernel, stride),\n",
    "          nn.BatchNorm2d(outChan),\n",
    "          nn.LeakyReLU(0.2, inplace = True),\n",
    "      )\n",
    "\n",
    "  def forward(self, image):\n",
    "    pred = self.discriminador(image)\n",
    "    return pred.view(len(pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DimImg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_472/3377655745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerator_noiseDim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_inChan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_dimensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDimImg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerador\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoiseDim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator_noiseDim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondDim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m131\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumChan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumChannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenDim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgen_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminador\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minChan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscriminator_inChan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhiddenDim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DimImg' is not defined"
     ]
    }
   ],
   "source": [
    "generator_noiseDim, discriminator_inChan = get_input_dimensions(z_dim, DimImg, numClases)\n",
    "\n",
    "gen = Generador(noiseDim=generator_noiseDim, condDim=131, numChan=numChannels, hiddenDim=64 ).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminador(inChan=discriminator_inChan,hiddenDim=64).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr) ##optimizar\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64*2\n",
    "display_step = 500\n",
    "batch_size = 12\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "#UNIT TEST NOTE: Initializations needed for grading\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size \n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        # Now you can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "       \n",
    "        #### START CODE HERE ####\n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        \n",
    "\n",
    "        # Now you can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels, \n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "        \n",
    "        #### START CODE HERE ####\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "        #\n",
    "\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WbRUCE0TK2fV"
   ],
   "name": "C1W4_2: Build a Conditional GAN (Student).ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "GANSC1-4A"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
